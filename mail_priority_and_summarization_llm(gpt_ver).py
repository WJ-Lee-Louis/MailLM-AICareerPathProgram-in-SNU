#####################################################
## MAIL PRIORIY AND SUMMARIZATION LLM(gpt-4o-mini) ##
#####################################################
from openai import OpenAI
import openai
import os
import streamlit as st
import backend

#######################################
## streamlit ì¸í„°í˜ì´ìŠ¤ ì›¹ì„œë¹„ìŠ¤ êµ¬í˜„ ##
#######################################
if "client" not in st.session_state:
    api_key = os.getenv('OPENAI_API_KEY')
    openai.api_key = api_key
    st.session_state['client']  = OpenAI()

def mail_priority_summarization_LLM_gpt(role, prompt):
    # Step1. ì—­í• ì§€ì •/ ì‚¬ì „ì •ì˜í•œ role ë³€ìˆ˜ë¥¼ ì¸ìë¡œ ë°›ì•„ ì‚¬ìš©
    prompt_list = [{'role':'system', 'content':role}]

    # Step2. êµ¬ì²´ì ì¸ prompt ì…ë ¥/ ì‚¬ì „ì •ì˜í•œ prompt ë³€ìˆ˜ë¥¼ ì¸ìë¡œ ë°›ì•„ ì‚¬ìš©
    prompt_list.append({'role':'user', 'content':prompt})

    # Step3. LLM ë‹µë³€ ìƒì„±
    completion = st.session_state['client'].chat.completions.create(
        model='gpt-4o-mini',
        messages=prompt_list,
        stream=True,
        max_tokens=10000
    )
    for c in completion:
        # ë§ˆì§€ë§‰ì— None ê°’ì´ delta contentë¡œ ë“¤ì–´ê°€ ìˆì–´ì„œ Noneì€ ì œì™¸í•˜ê³  ì¶œë ¥í•˜ê¸° ìœ„í•´ ifë¬¸ ì„¤ì •
        if c.choices[0].delta.content:
            # í•œ ê¸€ìì”© ì¶œë ¥í•˜ë˜, print ìë™ê°œí–‰ ì—†ì´ ì¶œë ¥
            # print(c.choices[0].delta.content ,end='')
            yield c.choices[0].delta.content # 1ê°œì˜ ë¬¸ìë¥¼ ì§€ì†ì ìœ¼ë¡œ return

st.set_page_config(page_title="Team2 Proj.", layout="wide")
st.title('ğŸ“§ LLM ê¸°ë°˜ ì‹ ê·œë©”ì¼ ì¤‘ìš”ë„ ë¶„ë¥˜ ë° ìš”ì•½ ì›¹ì„œë¹„ìŠ¤')
st.markdown("ì•ˆë…•í•˜ì„¸ìš”, ì‹ ì†í•œ ì—…ë¬´ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¤‘ìš” ë©”ì¼ë¶€í„° ìš”ì•½ê¹Œì§€ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# 'ìš°ì„ ìˆœìœ„ ì •ë ¬' ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ ë°”ë¡œ ë‹µë³€ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.
priority_clicked = st.button('ë©”ì¼ ë¶„ì„ ì‹œì‘ ğŸ‘†', use_container_width=True)

if priority_clicked:
    st.subheader('âœ…ì´ë©”ì¼ ë¶„ì„ ê²°ê³¼')
    with st.spinner('â³ ë‹¹ì‹ ì˜ ë©”ì¼í•¨ğŸ“ì— ì ‘ê·¼í•˜ì—¬ ë¶„ì„ì„ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤...'):
        output_container = st.empty()
        # mail_priority_summarization_LLM_gpt í•¨ìˆ˜ëŠ” generatorë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        # st.write_streamì´ ì´ generatorë¥¼ ë°›ì•„ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ì„ ìë™ìœ¼ë¡œ í™”ë©´ì— ì¶œë ¥í•´ì¤ë‹ˆë‹¤.
        response_generator = mail_priority_summarization_LLM_gpt(backend.role, backend.prompt)
        output_container.write_stream(response_generator)